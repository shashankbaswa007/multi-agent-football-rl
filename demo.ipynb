{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d148b04",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Multi-Agent Football RL - Interactive Demo\n",
    "# This notebook demonstrates training, evaluation, and visualization\n",
    "\n",
    "# Cell 1: Setup and Imports\n",
    "# ========================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, HTML\n",
    "import time\n",
    "\n",
    "from env.football_env import FootballEnv\n",
    "from models.ppo_agent import PPOAgent\n",
    "from training.buffer import MultiAgentBuffer\n",
    "from visualization.heatmap import HeatmapGenerator\n",
    "from visualization.pass_network import PassNetworkAnalyzer\n",
    "\n",
    "print(\"âœ“ Imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "\n",
    "# Cell 2: Create Environment\n",
    "# ===========================\n",
    "\n",
    "# Initialize environment\n",
    "env = FootballEnv(\n",
    "    num_agents_per_team=3,\n",
    "    grid_width=12,\n",
    "    grid_height=8,\n",
    "    max_steps=200,\n",
    "    render_mode='ansi'\n",
    ")\n",
    "\n",
    "# Get dimensions\n",
    "agent = env.agents[0]\n",
    "obs_dim = env.observation_space(agent).shape[0]\n",
    "action_dim = env.action_space(agent).n\n",
    "\n",
    "print(f\"Environment created!\")\n",
    "print(f\"Observation dim: {obs_dim}\")\n",
    "print(f\"Action dim: {action_dim}\")\n",
    "print(f\"Agents: {env.agents}\")\n",
    "\n",
    "# Visualize initial state\n",
    "observations, _ = env.reset()\n",
    "print(\"\\nInitial State:\")\n",
    "print(env._render_ansi())\n",
    "\n",
    "\n",
    "# Cell 3: Create Agents\n",
    "# ======================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create team agents\n",
    "team_0_agent = PPOAgent(\n",
    "    obs_dim=obs_dim,\n",
    "    action_dim=action_dim,\n",
    "    device=device,\n",
    "    lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95\n",
    ")\n",
    "\n",
    "team_1_agent = PPOAgent(\n",
    "    obs_dim=obs_dim,\n",
    "    action_dim=action_dim,\n",
    "    device=device,\n",
    "    lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95\n",
    ")\n",
    "\n",
    "print(\"âœ“ Agents created successfully!\")\n",
    "print(f\"Actor parameters: {sum(p.numel() for p in team_0_agent.actor.parameters()):,}\")\n",
    "print(f\"Critic parameters: {sum(p.numel() for p in team_0_agent.critic.parameters()):,}\")\n",
    "\n",
    "\n",
    "# Cell 4: Random Policy Baseline\n",
    "# ===============================\n",
    "\n",
    "def run_episode(env, policy='random'):\n",
    "    \"\"\"Run one episode with specified policy\"\"\"\n",
    "    observations, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while not done:\n",
    "        actions = {}\n",
    "        \n",
    "        for agent in env.agents:\n",
    "            obs = observations[agent]\n",
    "            \n",
    "            if policy == 'random':\n",
    "                action = env.action_space(agent).sample()\n",
    "            else:\n",
    "                # Use trained agent\n",
    "                team_id = 0 if 'team_0' in agent else 1\n",
    "                agent_obj = team_0_agent if team_id == 0 else team_1_agent\n",
    "                action, _, _, _ = agent_obj.get_action(obs, deterministic=True)\n",
    "                action = int(action)\n",
    "            \n",
    "            actions[agent] = action\n",
    "        \n",
    "        # Step environment\n",
    "        for agent in env.agent_iter():\n",
    "            observation, reward, termination, truncation, info = env.step(actions[agent])\n",
    "            observations[agent] = observation\n",
    "            \n",
    "            if 'team_0' in agent:\n",
    "                total_reward += reward\n",
    "            \n",
    "            if termination or truncation:\n",
    "                done = True\n",
    "                break\n",
    "        \n",
    "        steps += 1\n",
    "    \n",
    "    return total_reward, steps, env.episode_stats\n",
    "\n",
    "# Test random policy\n",
    "print(\"Testing random policy...\")\n",
    "rewards = []\n",
    "for i in range(10):\n",
    "    reward, steps, stats = run_episode(env, 'random')\n",
    "    rewards.append(reward)\n",
    "    if i == 0:\n",
    "        print(f\"\\nExample episode:\")\n",
    "        print(f\"  Total reward: {reward:.2f}\")\n",
    "        print(f\"  Steps: {steps}\")\n",
    "        print(f\"  Score: {stats['goals_team_0']}-{stats['goals_team_1']}\")\n",
    "        print(f\"  Passes: {stats['successful_passes']}/{stats['passes']}\")\n",
    "\n",
    "print(f\"\\nRandom policy average reward: {np.mean(rewards):.2f} Â± {np.std(rewards):.2f}\")\n",
    "\n",
    "\n",
    "# Cell 5: Training Loop (Mini Version)\n",
    "# ====================================\n",
    "\n",
    "def train_mini(num_episodes=100, update_interval=10):\n",
    "    \"\"\"Mini training loop for demonstration\"\"\"\n",
    "    \n",
    "    # Buffers\n",
    "    buffer_0 = MultiAgentBuffer(3, 2048, obs_dim)\n",
    "    buffer_1 = MultiAgentBuffer(3, 2048, obs_dim)\n",
    "    \n",
    "    rewards_history = []\n",
    "    win_rate_history = []\n",
    "    \n",
    "    print(\"Starting mini training...\")\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Collect episode\n",
    "        observations, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            actions = {}\n",
    "            \n",
    "            for agent in env.agents:\n",
    "                obs = observations[agent]\n",
    "                team_id = 0 if 'team_0' in agent else 1\n",
    "                agent_obj = team_0_agent if team_id == 0 else team_1_agent\n",
    "                \n",
    "                action, log_prob, value, _ = agent_obj.get_action(obs)\n",
    "                actions[agent] = action\n",
    "                \n",
    "                # Store in buffer\n",
    "                agent_idx = int(agent.split('_')[-1])\n",
    "                buffer = buffer_0 if team_id == 0 else buffer_1\n",
    "                buffer.add(agent_idx, obs, action, 0, value, log_prob, False)\n",
    "            \n",
    "            # Step\n",
    "            for agent in env.agent_iter():\n",
    "                observation, reward, termination, truncation, info = env.step(actions[agent])\n",
    "                observations[agent] = observation\n",
    "                \n",
    "                # Update buffer\n",
    "                team_id = 0 if 'team_0' in agent else 1\n",
    "                agent_idx = int(agent.split('_')[-1])\n",
    "                buffer = buffer_0 if team_id == 0 else buffer_1\n",
    "                buffer.buffers[agent_idx].rewards[buffer.buffers[agent_idx].ptr - 1] = reward\n",
    "                \n",
    "                if 'team_0' in agent:\n",
    "                    episode_reward += reward\n",
    "                \n",
    "                if termination or truncation:\n",
    "                    buffer.buffers[agent_idx].dones[buffer.buffers[agent_idx].ptr - 1] = 1\n",
    "                    done = True\n",
    "                    break\n",
    "        \n",
    "        rewards_history.append(episode_reward)\n",
    "        \n",
    "        # Update\n",
    "        if (episode + 1) % update_interval == 0:\n",
    "            # Compute returns\n",
    "            buffer_0.compute_returns_and_advantages([0, 0, 0])\n",
    "            buffer_1.compute_returns_and_advantages([0, 0, 0])\n",
    "            \n",
    "            # Update agents\n",
    "            from training.buffer import DummyBuffer\n",
    "            data_0 = buffer_0.get_all_training_data()\n",
    "            data_1 = buffer_1.get_all_training_data()\n",
    "            \n",
    "            team_0_agent.update(DummyBuffer(*data_0))\n",
    "            team_1_agent.update(DummyBuffer(*data_1))\n",
    "            \n",
    "            # Clear buffers\n",
    "            buffer_0.clear()\n",
    "            buffer_1.clear()\n",
    "            \n",
    "            # Evaluate\n",
    "            wins = 0\n",
    "            for _ in range(10):\n",
    "                _, _, stats = run_episode(env, 'trained')\n",
    "                if stats['goals_team_0'] > stats['goals_team_1']:\n",
    "                    wins += 1\n",
    "            \n",
    "            win_rate = wins / 10\n",
    "            win_rate_history.append(win_rate)\n",
    "            \n",
    "            # Print progress\n",
    "            avg_reward = np.mean(rewards_history[-update_interval:])\n",
    "            print(f\"Episode {episode+1}/{num_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.2f} | \"\n",
    "                  f\"Win Rate: {win_rate:.1%}\")\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "    \n",
    "    return rewards_history, win_rate_history\n",
    "\n",
    "# Run mini training\n",
    "rewards_hist, winrate_hist = train_mini(num_episodes=100, update_interval=10)\n",
    "\n",
    "print(\"\\nâœ“ Training complete!\")\n",
    "\n",
    "\n",
    "# Cell 6: Visualize Training Progress\n",
    "# ====================================\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Plot rewards\n",
    "ax1.plot(rewards_hist, alpha=0.3, label='Raw')\n",
    "window = 10\n",
    "smoothed = np.convolve(rewards_hist, np.ones(window)/window, mode='valid')\n",
    "ax1.plot(range(window-1, len(rewards_hist)), smoothed, label=f'Smoothed ({window})')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Total Reward')\n",
    "ax1.set_title('Training Reward')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot win rate\n",
    "episodes = [(i+1)*10 for i in range(len(winrate_hist))]\n",
    "ax2.plot(episodes, winrate_hist, marker='o')\n",
    "ax2.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='50% baseline')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Win Rate')\n",
    "ax2.set_title('Evaluation Win Rate')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Cell 7: Visualize Trained Agent\n",
    "# =================================\n",
    "\n",
    "print(\"Running trained agent...\")\n",
    "\n",
    "observations, _ = env.reset()\n",
    "done = False\n",
    "step = 0\n",
    "\n",
    "states = []\n",
    "\n",
    "while not done and step < 50:  # Limit to 50 steps for notebook\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Step {step}\")\n",
    "    print(env._render_ansi())\n",
    "    \n",
    "    # Save state\n",
    "    states.append({\n",
    "        'positions': env.agent_positions.copy(),\n",
    "        'ball': env.ball_position.copy()\n",
    "    })\n",
    "    \n",
    "    actions = {}\n",
    "    for agent in env.agents:\n",
    "        obs = observations[agent]\n",
    "        team_id = 0 if 'team_0' in agent else 1\n",
    "        agent_obj = team_0_agent if team_id == 0 else team_1_agent\n",
    "        action, _, _, _ = agent_obj.get_action(obs, deterministic=True)\n",
    "        actions[agent] = int(action)\n",
    "    \n",
    "    # Step\n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.step(actions[agent])\n",
    "        observations[agent] = observation\n",
    "        \n",
    "        if termination or truncation:\n",
    "            done = True\n",
    "            break\n",
    "    \n",
    "    step += 1\n",
    "    time.sleep(0.5)  # Slow down for visibility\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Episode Complete!\")\n",
    "print(f\"Final Score: {env.episode_stats['goals_team_0']}-{env.episode_stats['goals_team_1']}\")\n",
    "print(f\"Passes: {env.episode_stats['successful_passes']}/{env.episode_stats['passes']}\")\n",
    "\n",
    "\n",
    "# Cell 8: Analyze Behavior\n",
    "# =========================\n",
    "\n",
    "print(\"Analyzing learned behavior...\")\n",
    "\n",
    "# Run multiple episodes\n",
    "episode_data = []\n",
    "for i in range(20):\n",
    "    _, _, stats = run_episode(env, 'trained')\n",
    "    episode_data.append(stats)\n",
    "\n",
    "# Compute statistics\n",
    "avg_passes = np.mean([e['passes'] for e in episode_data])\n",
    "avg_successful = np.mean([e['successful_passes'] for e in episode_data])\n",
    "avg_shots = np.mean([e['shots'] for e in episode_data])\n",
    "wins = sum(1 for e in episode_data if e['goals_team_0'] > e['goals_team_1'])\n",
    "\n",
    "print(f\"\\nBehavior Analysis (20 episodes):\")\n",
    "print(f\"  Win rate: {wins/20:.1%}\")\n",
    "print(f\"  Avg passes: {avg_passes:.1f}\")\n",
    "print(f\"  Pass success rate: {avg_successful/avg_passes:.1%}\" if avg_passes > 0 else \"  No passes\")\n",
    "print(f\"  Avg shots: {avg_shots:.1f}\")\n",
    "print(f\"  Avg goals: {np.mean([e['goals_team_0'] for e in episode_data]):.1f}\")\n",
    "\n",
    "# Compare to random\n",
    "random_data = []\n",
    "for i in range(20):\n",
    "    _, _, stats = run_episode(env, 'random')\n",
    "    random_data.append(stats)\n",
    "\n",
    "random_passes = np.mean([e['passes'] for e in random_data])\n",
    "random_wins = sum(1 for e in random_data if e['goals_team_0'] > e['goals_team_1'])\n",
    "\n",
    "print(f\"\\nRandom Policy Baseline:\")\n",
    "print(f\"  Win rate: {random_wins/20:.1%}\")\n",
    "print(f\"  Avg passes: {random_passes:.1f}\")\n",
    "\n",
    "print(f\"\\nImprovement:\")\n",
    "print(f\"  Win rate: +{(wins-random_wins)/20*100:.1f}%\")\n",
    "print(f\"  Passes: +{avg_passes-random_passes:.1f}\")\n",
    "\n",
    "\n",
    "# Cell 9: Visualize Position Heatmap\n",
    "# ===================================\n",
    "\n",
    "# Collect position data\n",
    "position_data = {agent: [] for agent in env.agents}\n",
    "\n",
    "for _ in range(50):\n",
    "    observations, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        for agent, pos in env.agent_positions.items():\n",
    "            position_data[agent].append(pos.copy())\n",
    "        \n",
    "        actions = {}\n",
    "        for agent in env.agents:\n",
    "            obs = observations[agent]\n",
    "            team_id = 0 if 'team_0' in agent else 1\n",
    "            agent_obj = team_0_agent if team_id == 0 else team_1_agent\n",
    "            action, _, _, _ = agent_obj.get_action(obs, deterministic=True)\n",
    "            actions[agent] = int(action)\n",
    "        \n",
    "        for agent in env.agent_iter():\n",
    "            observation, reward, termination, truncation, info = env.step(actions[agent])\n",
    "            observations[agent] = observation\n",
    "            \n",
    "            if termination or truncation:\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "# Plot heatmaps\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, agent in enumerate(env.agents):\n",
    "    positions = np.array(position_data[agent])\n",
    "    \n",
    "    # Create 2D histogram\n",
    "    heatmap, xedges, yedges = np.histogram2d(\n",
    "        positions[:, 1], positions[:, 0],\n",
    "        bins=[env.grid_height, env.grid_width],\n",
    "        range=[[0, env.grid_height], [0, env.grid_width]]\n",
    "    )\n",
    "    \n",
    "    axes[idx].imshow(heatmap, origin='lower', cmap='YlOrRd', aspect='auto')\n",
    "    axes[idx].set_title(agent)\n",
    "    axes[idx].set_xlabel('X')\n",
    "    axes[idx].set_ylabel('Y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Heatmaps generated!\")\n",
    "\n",
    "\n",
    "# Cell 10: Summary and Next Steps\n",
    "# ================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DEMO COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ“Š Results Summary:\")\n",
    "print(f\"  - Trained for {len(rewards_hist)} episodes\")\n",
    "print(f\"  - Final win rate: {winrate_hist[-1]:.1%}\")\n",
    "print(f\"  - Learned to pass: {avg_passes:.1f} passes per episode\")\n",
    "print(f\"  - Pass success rate: {avg_successful/avg_passes:.1%}\" if avg_passes > 0 else \"\")\n",
    "\n",
    "print(\"\\nðŸš€ Next Steps:\")\n",
    "print(\"  1. Train for more episodes (10,000+) for better results\")\n",
    "print(\"  2. Use curriculum learning: python training/train_ppo.py --curriculum\")\n",
    "print(\"  3. Analyze pass networks: python visualization/pass_network.py\")\n",
    "print(\"  4. Experiment with reward shaping in football_env.py\")\n",
    "print(\"  5. Try self-play training against archived checkpoints\")\n",
    "\n",
    "print(\"\\nðŸ“š Resources:\")\n",
    "print(\"  - Full training: training/train_ppo.py\")\n",
    "print(\"  - Visualization: visualization/\")\n",
    "print(\"  - Config files: configs/\")\n",
    "print(\"  - Documentation: README.md\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Tips:\")\n",
    "print(\"  - Monitor training with TensorBoard: tensorboard --logdir runs/\")\n",
    "print(\"  - Save this trained model: torch.save(...)\")\n",
    "print(\"  - Load checkpoint: checkpoint = torch.load('model.pt')\")\n",
    "\n",
    "print(\"\\nHappy training! âš½ðŸ¤–\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
