# =============================================================================
# configs/emergency_fix.yaml - Emergency Fix for Training Stagnation
# =============================================================================
# CRITICAL FIXES:
# 1. Higher entropy (0.08 -> 0.02) to maintain exploration
# 2. Reduced time penalty in environment (code fix applied)
# 3. Observation and reward normalization enabled
# 4. Epsilon-greedy exploration (0.3 -> 0.05)
# 5. Adjusted hyperparameters for stable learning

# Environment settings - SIMPLIFIED to 1v1 for initial learning
num_agents_per_team: 1
grid_width: 10
grid_height: 6
max_steps: 100

# Training settings
num_episodes: 2000  # More episodes for 1v1 curriculum
update_interval: 4  # More frequent updates
buffer_size: 2048  # Smaller for 1v1
checkpoint_interval: 100

# Enhanced PPO hyperparameters - STABILITY FIXES
ppo_params:
  lr: 0.00005  # ðŸ”¥ CRITICAL: 6x reduction to prevent policy collapse
  gamma: 0.98  # Focus on near-term rewards
  gae_lambda: 0.95  # Balanced advantage estimation
  clip_epsilon: 0.3  # ðŸ”¥ Increased to allow necessary updates
  value_loss_coef: 2.0  # ðŸ”¥ DOUBLED for stronger value learning
  entropy_coef: 0.08  # ðŸ”¥ INCREASED to 8% for strong exploration
  max_grad_norm: 2.0  # ðŸ”¥ Increased to allow larger gradient steps
  ppo_epochs: 10  # Learning iterations per batch
  mini_batch_size: 64  # Smaller for 1v1

# Curriculum learning
curriculum: false

# Entropy decay - VERY SLOW for 1v1 curriculum
entropy_decay: true
entropy_decay_target: 0.02  # Keep higher minimum for exploration
entropy_decay_episodes: 8000  # Very slow decay

# Epsilon-greedy exploration - AGGRESSIVE
epsilon_start: 0.5  # Start with 50% random actions (very high!)
epsilon_end: 0.15  # End with 15% random actions
epsilon_decay_episodes: 8000  # Very slow decay

# Hardware
use_gpu: false

# Logging
log_dir: "runs/"
